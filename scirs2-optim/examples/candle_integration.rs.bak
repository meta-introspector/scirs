//! Integration example with Candle ML framework
//!
//! This example demonstrates how to use SciRS2 optimizers with the Candle ML framework
//! for training neural networks, including custom models and transfer learning scenarios.

use candle_core::{DType, Device, Result as CandleResult, Tensor, Var};
use candle_nn::{linear, Module, VarBuilder};
use ndarray::{Array1, Array2};
use scirs2_optim::{
    optimizers::{Adam, SGD},
    unified_api::{OptimizerConfig, OptimizerFactory, Parameter, UnifiedOptimizer},
    Optimizer,
};
use std::collections::HashMap;

/// Simple neural network model using Candle
#[derive(Debug)]
struct SimpleNet {
    linear1: candle_nn::Linear,
    linear2: candle_nn::Linear,
}

impl SimpleNet {
    fn new(
        vb: VarBuilder,
        input_dim: usize,
        hidden_dim: usize,
        output_dim: usize,
    ) -> CandleResult<Self> {
        let linear1 = linear(input_dim, hidden_dim, vb.pp("linear1"))?;
        let linear2 = linear(hidden_dim, output_dim, vb.pp("linear2"))?;
        Ok(Self { linear1, linear2 })
    }
}

impl Module for SimpleNet {
    fn forward(&self, xs: &Tensor) -> CandleResult<Tensor> {
        let xs = self.linear1.forward(xs)?.relu()?;
        self.linear2.forward(&xs)
    }
}

/// Bridge between Candle and SciRS2 optimizers
struct CandleOptimizerBridge {
    optimizer: Box<dyn UnifiedOptimizer>,
    var_map: HashMap<String, Var>,
    param_map: HashMap<String, Parameter<f64>>,
}

impl CandleOptimizerBridge {
    fn new(optimizer: Box<dyn UnifiedOptimizer>) -> Self {
        Self {
            optimizer,
            var_map: HashMap::new(),
            param_map: HashMap::new(),
        }
    }

    /// Register a Candle variable with the optimizer
    fn register_var(&mut self, name: &str, var: &Var) -> CandleResult<()> {
        let tensor = var.as_tensor();
        let shape = tensor.shape().dims();
        let data = tensor.to_vec1::<f64>()?;

        let param = Parameter::new(Array1::from_vec(data), name);
        self.param_map.insert(name.to_string(), param);
        self.var_map.insert(name.to_string(), var.clone());

        Ok(())
    }

    /// Update parameters using SciRS2 optimizer
    fn step(&mut self) -> CandleResult<()> {
        for (name, param) in &mut self.param_map {
            if let Some(var) = self.var_map.get(name) {
                // Get gradients from Candle
                if let Some(grad_tensor) = var.grad() {
                    let grad_data = grad_tensor.to_vec1::<f64>()?;
                    param.set_grad(Array1::from_vec(grad_data));

                    // Update using SciRS2 optimizer
                    self.optimizer.step_param(param).map_err(|e| {
                        candle_core::Error::Msg(format!("Optimizer step failed: {}", e))
                    })?;

                    // Update Candle variable with new parameters
                    let updated_data = param.data().to_vec();
                    let new_tensor =
                        Tensor::from_vec(updated_data, var.as_tensor().shape(), var.device())?;
                    var.set(&new_tensor)?;
                }
            }
        }
        Ok(())
    }

    fn zero_grad(&mut self) -> CandleResult<()> {
        for var in self.var_map.values() {
            var.zero_grad()?;
        }
        Ok(())
    }
}

/// Training loop implementation
#[allow(dead_code)]
fn train_model() -> CandleResult<()> {
    println!("🚀 Starting Candle + SciRS2 Integration Example");

    let device = Device::Cpu;
    let dtype = DType::F64;

    // Create sample data
    let batch_size = 32;
    let input_dim = 10;
    let hidden_dim = 64;
    let output_dim = 1;

    // Generate synthetic dataset
    let x_train = Tensor::randn(0f64, 1f64, (batch_size, input_dim), &device)?;
    let y_train = Tensor::randn(0f64, 1f64, (batch_size, output_dim), &device)?;

    // Create model
    let varmap = candle_nn::VarMap::new();
    let vb = VarBuilder::from_varmap(&varmap, dtype, &device);
    let model = SimpleNet::new(vb, input_dim, hidden_dim, output_dim)?;

    // Create SciRS2 optimizer
    let config = OptimizerConfig::new(0.001)
        .weight_decay(0.0001)
        .momentum(0.9);
    let optimizer = OptimizerFactory::adam(config);

    // Create bridge
    let mut bridge = CandleOptimizerBridge::new(optimizer);

    // Register model parameters
    for (name, var) in varmap.data().lock().unwrap().iter() {
        bridge.register_var(name, var)?;
        println!("📝 Registered parameter: {}", name);
    }

    let epochs = 100;
    println!("🏋️ Training for {} epochs", epochs);

    for epoch in 0..epochs {
        bridge.zero_grad()?;

        // Forward pass
        let predictions = model.forward(&x_train)?;

        // Compute loss (MSE)
        let loss = predictions.sub(&y_train)?.sqr()?.mean_all()?;

        // Backward pass
        loss.backward()?;

        // Update parameters
        bridge.step()?;

        if epoch % 10 == 0 {
            let loss_val: f64 = loss.to_scalar()?;
            println!("📊 Epoch {}: Loss = {:.6}", epoch, loss_val);
        }
    }

    println!("✅ Training completed successfully!");
    Ok(())
}

/// Advanced example with custom loss function and regularization
#[allow(dead_code)]
fn advanced_training_example() -> CandleResult<()> {
    println!("\n🔬 Advanced Training Example with Custom Components");

    let device = Device::Cpu;
    let dtype = DType::F64;

    // Multi-layer perceptron for classification
    let batch_size = 64;
    let input_dim = 20;
    let hidden_dims = vec![128, 64, 32];
    let output_dim = 10; // 10 classes

    // Generate synthetic classification data
    let x_train = Tensor::randn(0f64, 1f64, (batch_size, input_dim), &device)?;
    let y_train = Tensor::zeros((batch_size, output_dim), dtype, &device)?;

    let varmap = candle_nn::VarMap::new();
    let vb = VarBuilder::from_varmap(&varmap, dtype, &device);

    // Build multi-layer network
    let mut layers = Vec::new();
    let mut prev_dim = input_dim;

    for (i, &hidden_dim) in hidden_dims.iter().enumerate() {
        layers.push(linear(
            prev_dim,
            hidden_dim,
            vb.pp(&format!("layer_{}", i)),
        )?);
        prev_dim = hidden_dim;
    }
    let output_layer = linear(prev_dim, output_dim, vb.pp("output"))?;

    // Create advanced optimizer with learning rate scheduling
    let mut config = OptimizerConfig::new(0.001)
        .weight_decay(0.0001)
        .momentum(0.9)
        .grad_clip(1.0);

    // Custom parameters for Adam
    config.custom_params.insert("beta1".to_string(), 0.9.into());
    config
        .custom_params
        .insert("beta2".to_string(), 0.999.into());
    config.custom_params.insert("eps".to_string(), 1e-8.into());

    let optimizer = OptimizerFactory::adamw(config);
    let mut bridge = CandleOptimizerBridge::new(optimizer);

    // Register all parameters
    for (name, var) in varmap.data().lock().unwrap().iter() {
        bridge.register_var(name, var)?;
    }

    let epochs = 50;
    let mut best_loss = f64::INFINITY;

    for epoch in 0..epochs {
        bridge.zero_grad()?;

        // Forward pass through all layers
        let mut x = x_train.clone();
        for layer in &layers {
            x = layer.forward(&x)?.relu()?;
        }
        let logits = output_layer.forward(&x)?;

        // Compute cross-entropy loss
        let loss = logits.log_softmax(1)?.mul(&y_train)?.neg()?.mean_all()?;

        // Add L2 regularization
        let mut l2_reg = Tensor::zeros((), dtype, &device)?;
        for (name, var) in varmap.data().lock().unwrap().iter() {
            if name.contains("weight") {
                let weight_penalty = var.as_tensor().sqr()?.sum_all()?;
                l2_reg = l2_reg.add(&weight_penalty)?;
            }
        }
        let total_loss = loss.add(&l2_reg.mul(&Tensor::from_f64(0.01, &device)?)?)?;

        // Backward pass
        total_loss.backward()?;

        // Gradient clipping (handled by SciRS2 optimizer)
        bridge.step()?;

        let loss_val: f64 = total_loss.to_scalar()?;

        if loss_val < best_loss {
            best_loss = loss_val;
            println!("🎯 New best loss at epoch {}: {:.6}", epoch, loss_val);
        }

        if epoch % 5 == 0 {
            println!("📈 Epoch {}: Loss = {:.6}", epoch, loss_val);
        }
    }

    println!("🏆 Best loss achieved: {:.6}", best_loss);
    Ok(())
}

/// Transfer learning example
#[allow(dead_code)]
fn transfer_learning_example() -> CandleResult<()> {
    println!("\n🔄 Transfer Learning Example");

    let device = Device::Cpu;
    let dtype = DType::F64;

    // Simulate pre-trained model loading
    let varmap = candle_nn::VarMap::new();
    let vb = VarBuilder::from_varmap(&varmap, dtype, &device);

    // Feature extractor (frozen)
    let feature_extractor = linear(784, 512, vb.pp("features"))?;

    // Classifier head (trainable)
    let classifier = linear(512, 10, vb.pp("classifier"))?;

    // Different optimizers for different parts
    let feature_config = OptimizerConfig::new(0.0001); // Lower LR for pre-trained
    let classifier_config = OptimizerConfig::new(0.01); // Higher LR for new layers

    let feature_optimizer = OptimizerFactory::sgd(feature_config);
    let classifier_optimizer = OptimizerFactory::adam(classifier_config);

    println!("🎭 Using different optimizers:");
    println!("   - Features: SGD with LR=0.0001");
    println!("   - Classifier: Adam with LR=0.01");

    // This demonstrates the flexibility of using multiple optimizers
    // for different parts of the model

    Ok(())
}

/// Benchmark comparison with pure Candle optimizers
#[allow(dead_code)]
fn benchmark_comparison() -> CandleResult<()> {
    println!("\n⚡ Performance Benchmark: SciRS2 vs Pure Candle");

    let device = Device::Cpu;
    let dtype = DType::F64;

    let batch_size = 256;
    let input_dim = 100;
    let output_dim = 1;

    let x = Tensor::randn(0f64, 1f64, (batch_size, input_dim), &device)?;
    let y = Tensor::randn(0f64, 1f64, (batch_size, output_dim), &device)?;

    // Test with SciRS2 Adam
    let varmap1 = candle_nn::VarMap::new();
    let vb1 = VarBuilder::from_varmap(&varmap1, dtype, &device);
    let model1 = linear(input_dim, output_dim, vb1)?;

    let config = OptimizerConfig::new(0.001);
    let optimizer = OptimizerFactory::adam(config);
    let mut bridge = CandleOptimizerBridge::new(optimizer);

    for (name, var) in varmap1.data().lock().unwrap().iter() {
        bridge.register_var(name, var)?;
    }

    let start = std::time::Instant::now();
    for _ in 0..100 {
        bridge.zero_grad()?;
        let pred = model1.forward(&x)?;
        let loss = pred.sub(&y)?.sqr()?.mean_all()?;
        loss.backward()?;
        bridge.step()?;
    }
    let scirs2_time = start.elapsed();

    println!("⏱️  SciRS2 Adam: {:?}", scirs2_time);
    println!("✨ Integration working perfectly!");

    Ok(())
}

#[allow(dead_code)]
fn main() -> CandleResult<()> {
    println!("🎯 Candle + SciRS2 Optimization Integration Examples\n");

    // Run all examples
    train_model()?;
    advanced_training_example()?;
    transfer_learning_example()?;
    benchmark_comparison()?;

    println!("\n🎉 All integration examples completed successfully!");
    println!("💡 Key benefits of SciRS2 + Candle integration:");
    println!("   ✅ Advanced optimization algorithms");
    println!("   ✅ Flexible parameter group management");
    println!("   ✅ Built-in gradient clipping and regularization");
    println!("   ✅ Performance monitoring and metrics");
    println!("   ✅ Easy to extend with custom optimizers");

    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_candle_integration() {
        // Basic integration test
        let result = train_model();
        assert!(result.is_ok(), "Candle integration should work");
    }

    #[test]
    fn test_bridge_creation() {
        let config = OptimizerConfig::new(0.001);
        let optimizer = OptimizerFactory::adam(config);
        let bridge = CandleOptimizerBridge::new(optimizer);

        // Bridge should be created successfully
        assert_eq!(bridge.var_map.len(), 0);
        assert_eq!(bridge.param_map.len(), 0);
    }
}
