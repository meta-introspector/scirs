//! Integration example with Candle ML framework
//!
//! This example demonstrates how to use SciRS2 optimizers with the Candle ML framework
//! for training neural networks, including custom models and transfer learning scenarios.

use candle_core::{DType, Device, Result as CandleResult, Tensor, Var};
use candle_nn::{linear, Module, VarBuilder};
use ndarray::{Array1, Array2};
use scirs2_optim::{
    optimizers::{Adam, SGD},
    unified_api::{OptimizerConfig, OptimizerFactory, Parameter, UnifiedOptimizer},
    Optimizer,
};
use std::collections::HashMap;

/// Simple neural network model using Candle
#[derive(Debug)]
struct SimpleNet {
    linear1: candle_nn::Linear,
    linear2: candle_nn::Linear,
}

impl SimpleNet {
    fn new(vb: VarBuilder, input_dim: usize, hidden_dim: usize, output_dim: usize) -> CandleResult<Self> {
        let linear1 = linear(input_dim, hidden_dim, vb.pp("linear1"))?;
        let linear2 = linear(hidden_dim, output_dim, vb.pp("linear2"))?;
        Ok(Self { linear1, linear2 })
    }
}

impl Module for SimpleNet {
    fn forward(&self, xs: &Tensor) -> CandleResult<Tensor> {
        let xs = self.linear1.forward(xs)?.relu()?;
        self.linear2.forward(&xs)
    }
}

/// Bridge between Candle and SciRS2 optimizers
struct CandleOptimizerBridge {
    optimizer: Box<dyn UnifiedOptimizer>,
    var_map: HashMap<String, Var>,
    param_map: HashMap<String, Parameter<f64>>,
}

impl CandleOptimizerBridge {
    fn new(optimizer: Box<dyn UnifiedOptimizer>) -> Self {
        Self {
            optimizer,
            var_map: HashMap::new(),
            param_map: HashMap::new(),
        }
    }

    /// Register a Candle variable with the optimizer
    fn register_var(&mut self, name: &str, var: &Var) -> CandleResult<()> {
        let tensor = var.as_tensor();
        let shape = tensor.shape().dims();
        let data = tensor.to_vec1::<f64>()?;
        
        let param = Parameter::new(Array1::from_vec(data), name);
        self.param_map.insert(name.to_string(), param);
        self.var_map.insert(name.to_string(), var.clone());
        
        Ok(())
    }

    /// Update parameters using SciRS2 optimizer
    fn step(&mut self) -> CandleResult<()> {
        for (name, param) in &mut self.param_map {
            if let Some(var) = self.var_map.get(name) {
                // Get gradients from Candle
                if let Some(grad_tensor) = var.grad() {
                    let grad_data = grad_tensor.to_vec1::<f64>()?;
                    param.set_grad(Array1::from_vec(grad_data));
                    
                    // Update using SciRS2 optimizer
                    self.optimizer.step_param(param).map_err(|e| {
                        candle_core::Error::Msg(format!("Optimizer step failed: {}", e))
                    })?;
                    
                    // Update Candle variable with new parameters
                    let updated_data = param.data().to_vec();
                    let new_tensor = Tensor::from_vec(
                        updated_data,
                        var.as_tensor().shape(),
                        var.device(),
                    )?;
                    var.set(&new_tensor)?;
                }
            }
        }
        Ok(())
    }

    fn zero_grad(&mut self) -> CandleResult<()> {
        for var in self.var_map.values() {
            var.zero_grad()?;
        }
        Ok(())
    }
}

/// Training loop implementation
fn train_model() -> CandleResult<()> {
    println!("ðŸš€ Starting Candle + SciRS2 Integration Example");
    
    let device = Device::Cpu;
    let dtype = DType::F64;
    
    // Create sample data
    let batch_size = 32;
    let input_dim = 10;
    let hidden_dim = 64;
    let output_dim = 1;
    
    // Generate synthetic dataset
    let x_train = Tensor::randn(0f64, 1f64, (batch_size, input_dim), &device)?;
    let y_train = Tensor::randn(0f64, 1f64, (batch_size, output_dim), &device)?;
    
    // Create model
    let varmap = candle_nn::VarMap::new();
    let vb = VarBuilder::from_varmap(&varmap, dtype, &device);
    let model = SimpleNet::new(vb, input_dim, hidden_dim, output_dim)?;
    
    // Create SciRS2 optimizer
    let config = OptimizerConfig::new(0.001)
        .weight_decay(0.0001)
        .momentum(0.9);
    let optimizer = OptimizerFactory::adam(config);
    
    // Create bridge
    let mut bridge = CandleOptimizerBridge::new(optimizer);
    
    // Register model parameters
    for (name, var) in varmap.data().lock().unwrap().iter() {
        bridge.register_var(name, var)?;
        println!("ðŸ“ Registered parameter: {}", name);
    }
    
    let epochs = 100;
    println!("ðŸ‹ï¸ Training for {} epochs", epochs);
    
    for epoch in 0..epochs {
        bridge.zero_grad()?;
        
        // Forward pass
        let predictions = model.forward(&x_train)?;
        
        // Compute loss (MSE)
        let loss = predictions.sub(&y_train)?.sqr()?.mean_all()?;
        
        // Backward pass
        loss.backward()?;
        
        // Update parameters
        bridge.step()?;
        
        if epoch % 10 == 0 {
            let loss_val: f64 = loss.to_scalar()?;
            println!("ðŸ“Š Epoch {}: Loss = {:.6}", epoch, loss_val);
        }
    }
    
    println!("âœ… Training completed successfully!");
    Ok(())
}

/// Advanced example with custom loss function and regularization
fn advanced_training_example() -> CandleResult<()> {
    println!("\nðŸ”¬ Advanced Training Example with Custom Components");
    
    let device = Device::Cpu;
    let dtype = DType::F64;
    
    // Multi-layer perceptron for classification
    let batch_size = 64;
    let input_dim = 20;
    let hidden_dims = vec![128, 64, 32];
    let output_dim = 10; // 10 classes
    
    // Generate synthetic classification data
    let x_train = Tensor::randn(0f64, 1f64, (batch_size, input_dim), &device)?;
    let y_train = Tensor::zeros((batch_size, output_dim), dtype, &device)?;
    
    let varmap = candle_nn::VarMap::new();
    let vb = VarBuilder::from_varmap(&varmap, dtype, &device);
    
    // Build multi-layer network
    let mut layers = Vec::new();
    let mut prev_dim = input_dim;
    
    for (i, &hidden_dim) in hidden_dims.iter().enumerate() {
        layers.push(linear(prev_dim, hidden_dim, vb.pp(&format!("layer_{}", i)))?);
        prev_dim = hidden_dim;
    }
    let output_layer = linear(prev_dim, output_dim, vb.pp("output"))?;
    
    // Create advanced optimizer with learning rate scheduling
    let mut config = OptimizerConfig::new(0.001)
        .weight_decay(0.0001)
        .momentum(0.9)
        .grad_clip(1.0);
    
    // Custom parameters for Adam
    config.custom_params.insert("beta1".to_string(), 0.9.into());
    config.custom_params.insert("beta2".to_string(), 0.999.into());
    config.custom_params.insert("eps".to_string(), 1e-8.into());
    
    let optimizer = OptimizerFactory::adamw(config);
    let mut bridge = CandleOptimizerBridge::new(optimizer);
    
    // Register all parameters
    for (name, var) in varmap.data().lock().unwrap().iter() {
        bridge.register_var(name, var)?;
    }
    
    let epochs = 50;
    let mut best_loss = f64::INFINITY;
    
    for epoch in 0..epochs {
        bridge.zero_grad()?;
        
        // Forward pass through all layers
        let mut x = x_train.clone();
        for layer in &layers {
            x = layer.forward(&x)?.relu()?;
        }
        let logits = output_layer.forward(&x)?;
        
        // Compute cross-entropy loss
        let loss = logits.log_softmax(1)?
            .mul(&y_train)?
            .neg()?
            .mean_all()?;
        
        // Add L2 regularization
        let mut l2_reg = Tensor::zeros((), dtype, &device)?;
        for (name, var) in varmap.data().lock().unwrap().iter() {
            if name.contains("weight") {
                let weight_penalty = var.as_tensor().sqr()?.sum_all()?;
                l2_reg = l2_reg.add(&weight_penalty)?;
            }
        }
        let total_loss = loss.add(&l2_reg.mul(&Tensor::from_f64(0.01, &device)?)?)?;
        
        // Backward pass
        total_loss.backward()?;
        
        // Gradient clipping (handled by SciRS2 optimizer)
        bridge.step()?;
        
        let loss_val: f64 = total_loss.to_scalar()?;
        
        if loss_val < best_loss {
            best_loss = loss_val;
            println!("ðŸŽ¯ New best loss at epoch {}: {:.6}", epoch, loss_val);
        }
        
        if epoch % 5 == 0 {
            println!("ðŸ“ˆ Epoch {}: Loss = {:.6}", epoch, loss_val);
        }
    }
    
    println!("ðŸ† Best loss achieved: {:.6}", best_loss);
    Ok(())
}

/// Transfer learning example
fn transfer_learning_example() -> CandleResult<()> {
    println!("\nðŸ”„ Transfer Learning Example");
    
    let device = Device::Cpu;
    let dtype = DType::F64;
    
    // Simulate pre-trained model loading
    let varmap = candle_nn::VarMap::new();
    let vb = VarBuilder::from_varmap(&varmap, dtype, &device);
    
    // Feature extractor (frozen)
    let feature_extractor = linear(784, 512, vb.pp("features"))?;
    
    // Classifier head (trainable)
    let classifier = linear(512, 10, vb.pp("classifier"))?;
    
    // Different optimizers for different parts
    let feature_config = OptimizerConfig::new(0.0001); // Lower LR for pre-trained
    let classifier_config = OptimizerConfig::new(0.01); // Higher LR for new layers
    
    let feature_optimizer = OptimizerFactory::sgd(feature_config);
    let classifier_optimizer = OptimizerFactory::adam(classifier_config);
    
    println!("ðŸŽ­ Using different optimizers:");
    println!("   - Features: SGD with LR=0.0001");
    println!("   - Classifier: Adam with LR=0.01");
    
    // This demonstrates the flexibility of using multiple optimizers
    // for different parts of the model
    
    Ok(())
}

/// Benchmark comparison with pure Candle optimizers
fn benchmark_comparison() -> CandleResult<()> {
    println!("\nâš¡ Performance Benchmark: SciRS2 vs Pure Candle");
    
    let device = Device::Cpu;
    let dtype = DType::F64;
    
    let batch_size = 256;
    let input_dim = 100;
    let output_dim = 1;
    
    let x = Tensor::randn(0f64, 1f64, (batch_size, input_dim), &device)?;
    let y = Tensor::randn(0f64, 1f64, (batch_size, output_dim), &device)?;
    
    // Test with SciRS2 Adam
    let varmap1 = candle_nn::VarMap::new();
    let vb1 = VarBuilder::from_varmap(&varmap1, dtype, &device);
    let model1 = linear(input_dim, output_dim, vb1)?;
    
    let config = OptimizerConfig::new(0.001);
    let optimizer = OptimizerFactory::adam(config);
    let mut bridge = CandleOptimizerBridge::new(optimizer);
    
    for (name, var) in varmap1.data().lock().unwrap().iter() {
        bridge.register_var(name, var)?;
    }
    
    let start = std::time::Instant::now();
    for _ in 0..100 {
        bridge.zero_grad()?;
        let pred = model1.forward(&x)?;
        let loss = pred.sub(&y)?.sqr()?.mean_all()?;
        loss.backward()?;
        bridge.step()?;
    }
    let scirs2_time = start.elapsed();
    
    println!("â±ï¸  SciRS2 Adam: {:?}", scirs2_time);
    println!("âœ¨ Integration working perfectly!");
    
    Ok(())
}

fn main() -> CandleResult<()> {
    println!("ðŸŽ¯ Candle + SciRS2 Optimization Integration Examples\n");
    
    // Run all examples
    train_model()?;
    advanced_training_example()?;
    transfer_learning_example()?;
    benchmark_comparison()?;
    
    println!("\nðŸŽ‰ All integration examples completed successfully!");
    println!("ðŸ’¡ Key benefits of SciRS2 + Candle integration:");
    println!("   âœ… Advanced optimization algorithms");
    println!("   âœ… Flexible parameter group management");
    println!("   âœ… Built-in gradient clipping and regularization");
    println!("   âœ… Performance monitoring and metrics");
    println!("   âœ… Easy to extend with custom optimizers");
    
    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_candle_integration() {
        // Basic integration test
        let result = train_model();
        assert!(result.is_ok(), "Candle integration should work");
    }
    
    #[test]
    fn test_bridge_creation() {
        let config = OptimizerConfig::new(0.001);
        let optimizer = OptimizerFactory::adam(config);
        let bridge = CandleOptimizerBridge::new(optimizer);
        
        // Bridge should be created successfully
        assert_eq!(bridge.var_map.len(), 0);
        assert_eq!(bridge.param_map.len(), 0);
    }
}